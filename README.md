
/data/letters - папка с распакованым датасетом (notMNIST small или large). Должна содержать папки A, B, C, ... J с изображениями соответствующих букв

Данные используются для обеих лабораторных работ

# Лабораторная работа 1

[/lab1.ipynb](/lab1.ipynb) - jupyter notebook с лабораторной работой

## Алгоритм
1. Данные загружаются из файлов, есть пара файлов, которые не удается открыть или не удается считать изображение (пропуски в данных), такие данные пропускаются, так как мы никак не можем обработать случай отсутствия изображения.
2. Отображается несколько букв в соответствии с _Заданием 1_
3. Отображаются размеры каждого класса после загрузки в соответствии с _Заданием 2_
4. Удаляются дубликаты из выборки в соответствии с _Заданием 4_ (также там говорится про проверку дубликатов между тестовой и обучающей выборкой, однако для этого используется функция `train_test_split` библиотеки `scikit-learn`, и она не допускает дубликатов в выборках)
5. Строится 10 классификаторов размером alpha * <размер датасета>, где alpha выбираются равномерно из диапазона [0.05, 0.99]. Строится график зависимости точности (accuracy) классификатора в зависимости от размера датасета.

# Лабораторная работа 2

[/lab2.ipynb](/lab2.ipynb) - jupyter notebook с лабораторной работой

Строится три модели в соответствии с условием:
1. Простейшая нейронная сеть. Была выбрана архитектура сети со следующими слоями:
    - Входной слой - 256 нейронов, функция активации - RELU
    - 1-ый скрытый слой - 128 нейронов, RELU
    - 2-ой скрытый слой - 64 нейрона, RELU
    - Выходной слой - функция активации - softmax, так как имеем дело с категоризацией
2. Нейронная сеть с заданным Dropout = 0.3
3. Нейронная сеть с адаптивным learning rate. В качестве функции изменения learning rate используется экспоненциальная зависимость

# Лабораторная работа 3 (Лабораторная работа 1 2-го семестра)

[/lab3.ipynb](/lab3.ipynb) - jupyter notebook с лабораторной работой

Каждое отдельное задание отмечено комментарием. Выведены логи обучения и построены графики для функции потерь и точности предсказаний
